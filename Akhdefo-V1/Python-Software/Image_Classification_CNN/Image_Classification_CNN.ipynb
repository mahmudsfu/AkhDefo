{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d34e5115",
   "metadata": {},
   "source": [
    "# Train the CNN Machine Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd69ef38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from mahmud_ml.lenet import LeNet\n",
    "from imutils import paths\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import random\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "# construct the argument parse and parse the arguments\n",
    "#ap = argparse.ArgumentParser()\n",
    "#ap.add_argument(\"-d\", \"--dataset\", required=True,\n",
    "#  help=\"path to input dataset\")\n",
    "#ap.add_argument(\"-m\", \"--model\", required=True,\n",
    " # help=\"path to output model\")\n",
    "#ap.add_argument(\"-p\", \"--plot\", type=str, default=\"plot.png\",\n",
    " # help=\"path to output loss/accuracy plot\")\n",
    "#args = vars(ap.parse_args())\n",
    "args = {\n",
    "\t\"dataset\": \"DataForTraining\",\n",
    "\t\"model\": \"foggy_not_foggy.model\",\n",
    "    \"plot\": \"plot.png\"\n",
    "}\n",
    "\n",
    "\n",
    "# initialize the number of epochs to train for, initia learning rate,\n",
    "# and batch size\n",
    "EPOCHS = 200\n",
    "INIT_LR = 1e-3\n",
    "BS = 32\n",
    "\n",
    "# initialize the data and labels\n",
    "print(\"[INFO] loading images...\")\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "# grab the image paths and randomly shuffle\n",
    "imagePaths = sorted(list(paths.list_images(args[\"dataset\"])))\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(imagePaths)\n",
    "\n",
    "# loop over the input images\n",
    "for imagePath in imagePaths:\n",
    "\t# load the image, pre-process it, and store it in the data list\n",
    "\timage = cv2.imread(imagePath)\n",
    "\t#image = cv2.resize(image, (28, 28))\n",
    "\n",
    "\t#resize image without distortion\n",
    "\timg=image\n",
    "\tsize=(28,28)\n",
    "\th, w = img.shape[:2]\n",
    "\tc = img.shape[2] if len(img.shape)>2 else 1\n",
    "\n",
    "\tif h == w:\n",
    "\t\tcv2.resize(img, size, cv2.INTER_AREA)\n",
    "\n",
    "\tdif = h if h > w else w\n",
    "\n",
    "\tinterpolation = cv2.INTER_AREA if dif > (size[0]+size[1])//2 else  cv2.INTER_CUBIC\n",
    "\n",
    "\tx_pos = (dif - w)//2\n",
    "\ty_pos = (dif - h)//2\n",
    "\n",
    "\tif len(img.shape) == 2:\n",
    "\t\tmask = np.zeros((dif, dif), dtype=img.dtype)\n",
    "\t\tmask[y_pos:y_pos+h, x_pos:x_pos+w] = img[:h, :w]\n",
    "\telse:\n",
    "\t\tmask = np.zeros((dif, dif, c), dtype=img.dtype)\n",
    "\t\tmask[y_pos:y_pos+h, x_pos:x_pos+w, :] = img[:h, :w, :]\n",
    "\t\timage=cv2.resize(mask, size, interpolation)\n",
    "\n",
    "\t########\n",
    "\t\n",
    "\timage = img_to_array(image)\n",
    "\tdata.append(image)\n",
    "\n",
    "\t# extract the class label from the image path and update the\n",
    "\t# labels list\n",
    "\tlabel = imagePath.split(os.path.sep)[-2]\n",
    "    \n",
    "    \n",
    "\tlabel = 1 if label == \"foggy\" else 0\n",
    "\tlabels.append(label)\n",
    "\n",
    "# scale the raw pixel intensities to the range [0, 1]\n",
    "data = np.array(data, dtype=\"float\") / 255.0\n",
    "labels = np.array(labels)\n",
    "\n",
    "# partition the data into training and testing splits using 75% of\n",
    "# the data for training and the remaining 25% for testing\n",
    "(trainX, testX, trainY, testY) = train_test_split(data,\n",
    "\tlabels, test_size=0.25, random_state=42)\n",
    "\n",
    "# convert the labels from integers to vectors\n",
    "trainY = to_categorical(trainY, num_classes=2)\n",
    "testY = to_categorical(testY, num_classes=2)\n",
    "\n",
    "# construct the image generator for data augmentation\n",
    "aug = ImageDataGenerator(rotation_range=30, width_shift_range=0.1,\n",
    "\theight_shift_range=0.1, shear_range=0.2, zoom_range=0.2,\n",
    "\thorizontal_flip=True, fill_mode=\"nearest\")\n",
    "\n",
    "# initialize the model\n",
    "print(\"[INFO] compiling model...\")\n",
    "model = LeNet.build(width=28, height=28, depth=3, classes=2)\n",
    "opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=opt,\n",
    "\tmetrics=[\"accuracy\"])\n",
    "\n",
    "# train the network\n",
    "print(\"[INFO] training network...\")\n",
    "H = model.fit(x=aug.flow(trainX, trainY, batch_size=BS),\n",
    "\tvalidation_data=(testX, testY), steps_per_epoch=len(trainX) // BS,\n",
    "\tepochs=EPOCHS, verbose=1)\n",
    "\n",
    "# save the model to disk\n",
    "print(\"[INFO] serializing network...\")\n",
    "model.save(args[\"model\"], save_format=\"h5\")\n",
    "\n",
    "\n",
    "# plot the training loss and accuracy\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "N = EPOCHS\n",
    "plt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(np.arange(0, N), H.history[\"accuracy\"], label=\"train_acc\")\n",
    "plt.plot(np.arange(0, N), H.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy on fog/Not fog\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.savefig(args[\"plot\"])\n",
    "# show the output image\n",
    "img=(\"plot.png\")\n",
    "img=plt.imread(img)\n",
    "#imgplot = plt.imshow(img)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddc4a55",
   "metadata": {},
   "source": [
    "# Use the trained Model to classify clear images from noisy and  foggy images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8910a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import the necessary packages\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import argparse\n",
    "import cv2\n",
    "import glob\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import numpy\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "import os\n",
    "\n",
    "#setting required file directories\n",
    "if not os.path.exists('filtered_images_noFog'):\n",
    "    os.makedirs('filtered_images_noFog')\n",
    "    \n",
    "if not os.path.exists('filtered_images_Fog'):\n",
    "    os.makedirs('filtered_images_Fog')\n",
    "\n",
    "if not os.path.exists('ClearImages_daily'):\n",
    "    os.makedirs('ClearImages_daily')\n",
    "\n",
    "No_Fogg_path=\"filtered_images_noFog\"\n",
    "Foggy_Path=\"filtered_images_Fog\"\n",
    "dailyimages=\"ClearImages_daily\"\n",
    "########\n",
    "args = {\n",
    "\t\"model\": \"foggy_not_foggy.model\",\n",
    "\t#\"input\": join(mypath,onlyfiles[n])\n",
    "    }\n",
    "#mypath= r'dataset_imagery'\n",
    "mypath = os.getcwd() + \"\\\\dataset_imagery\"\n",
    "\n",
    "onlyfiles = [ f for f in listdir(mypath) if isfile(join(mypath,f)) ]\n",
    "images = numpy.empty(len(onlyfiles), dtype=object)\n",
    "#Run the program\n",
    "output_dir=r\"figs\"\n",
    "\n",
    "for n in range(0, len(onlyfiles)):\n",
    "    print (\"Now Processing Image Number\", n+1 )\n",
    "\n",
    "    #images[n] =  join(mypath,onlyfiles[n])\n",
    "        #image = cv2.imread(images[n])\n",
    "        #im_med = ndimage.median_filter(image, 5)\n",
    "    image = cv2.imread(join(mypath,onlyfiles[n]))\n",
    "    orig = image.copy()\n",
    "  # pre-process the image for classification\n",
    "    image= cv2.resize(image, (28, 28))\n",
    "    image = img_to_array(image)\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    image = image.astype(\"float\") / 255.0\n",
    "    print (join(mypath,onlyfiles[n]))\n",
    "  # load the trained convolutional neural network\n",
    "    print(\"[INFO] loading network...\", onlyfiles[n])\n",
    "    model = load_model(args[\"model\"])\n",
    "       \n",
    "    # classify the input image\n",
    "    (not_foggy, foggy) = model.predict(image)[0]\n",
    "    label = \"NotFoggy\" if not_foggy  > foggy else \"foggy\"\n",
    "    proba = not_foggy  if not_foggy > foggy  else foggy \n",
    "    label = \"{}: {:.2f}%\".format(label, proba * 100)\n",
    "  #print (label)\n",
    "  #output = imutils.resize(orig, width=1600)\n",
    "    cv2.putText(orig, label, (40, 100),  cv2.FONT_HERSHEY_SIMPLEX,0.7, (0, 255, 0), 2)\n",
    "    im = Image.fromarray(orig)\n",
    "    \n",
    "    file = str(onlyfiles[n])\n",
    "\n",
    "    position = file.index(\".jpg\")\n",
    "\n",
    "    filename = file[0:position]\n",
    "    \n",
    "    if (proba > 0.95 and not_foggy  > foggy):\n",
    "        label=\"Not Foggy\"\n",
    "      #im.save(No_Fogg_path+'//' +onlyfiles[n]+'-'+str(proba)+'.jpeg') \n",
    "        im.save(No_Fogg_path+'//' +str(proba)+'-'+filename+'.jpg') \n",
    "        im.save(dailyimages+ '//' +filename+ '.jpg')\n",
    "      \n",
    "\n",
    "    else:\n",
    "        label=\"Foggy\"\n",
    "      #im.save(Foggy_Path+'//' +onlyfiles[n]+'-'+str(proba)+'.jpeg') \n",
    "        im.save(Foggy_Path+'//' +str(proba)+'-'+filename+'.jpg') \n",
    "      \n",
    "\n",
    "        #imgplot = plt.imshow(im)\n",
    "        plt.show() \n",
    "        continue\n",
    "\n",
    "print (\"No more files left to process\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6786965a",
   "metadata": {},
   "source": [
    "# Retrive Hourly Images and put them into different folders based the time of the day the phtos were taken such as 19(11am),20(12pm),21(1pm) and 22(2pm) Oclock. Note times are in UTC format.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9662240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "16\n",
      "Do not save those images\n",
      "17\n",
      "Do not save those images\n",
      "18\n",
      "Do not save those images\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "Do not save those images\n",
      "00\n",
      "Do not save those images\n",
      "01\n",
      "Do not save those images\n",
      "16\n",
      "Do not save those images\n",
      "17\n",
      "Do not save those images\n",
      "18\n",
      "Do not save those images\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "Do not save those images\n",
      "00\n",
      "Do not save those images\n",
      "01\n",
      "Do not save those images\n",
      "17\n",
      "Do not save those images\n",
      "18\n",
      "Do not save those images\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "Do not save those images\n",
      "00\n",
      "Do not save those images\n",
      "01\n",
      "Do not save those images\n",
      "17\n",
      "Do not save those images\n",
      "18\n",
      "Do not save those images\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "Do not save those images\n",
      "00\n",
      "Do not save those images\n",
      "18\n",
      "Do not save those images\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "Do not save those images\n",
      "00\n",
      "Do not save those images\n",
      "17\n",
      "Do not save those images\n",
      "18\n",
      "Do not save those images\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "Do not save those images\n",
      "00\n",
      "Do not save those images\n",
      "17\n",
      "Do not save those images\n",
      "18\n",
      "Do not save those images\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "Do not save those images\n",
      "00\n",
      "Do not save those images\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "Do not save those images\n",
      "20\n",
      "00\n",
      "Do not save those images\n",
      "18\n",
      "Do not save those images\n",
      "19\n",
      "21\n",
      "22\n",
      "23\n",
      "Do not save those images\n",
      "00\n",
      "Do not save those images\n",
      "00\n",
      "Do not save those images\n",
      "19\n",
      "20\n",
      "22\n",
      "20\n",
      "21\n",
      "22\n",
      "00\n",
      "Do not save those images\n",
      "18\n",
      "Do not save those images\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "Do not save those images\n",
      "00\n",
      "Do not save those images\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "Do not save those images\n",
      "00\n",
      "Do not save those images\n",
      "18\n",
      "Do not save those images\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "Do not save those images\n",
      "00\n",
      "Do not save those images\n",
      "18\n",
      "Do not save those images\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "00\n",
      "Do not save those images\n",
      "18\n",
      "Do not save those images\n",
      "00\n",
      "Do not save those images\n",
      "18\n",
      "Do not save those images\n",
      "20\n",
      "21\n",
      "23\n",
      "Do not save those images\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "Do not save those images\n",
      "18\n",
      "Do not save those images\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "Do not save those images\n",
      "00\n",
      "Do not save those images\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "Do not save those images\n",
      "18\n",
      "Do not save those images\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "Do not save those images\n",
      "18\n",
      "Do not save those images\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "Do not save those images\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "Do not save those images\n",
      "19\n",
      "19\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "Do not save those images\n",
      "20\n",
      "21\n",
      "22\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "Do not save those images\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "Do not save those images\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "Do not save those images\n",
      "22\n",
      "20\n",
      "22\n",
      "23\n",
      "Do not save those images\n",
      "21\n",
      "task is finished\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import matplotlib.colors\n",
    "from matplotlib import pyplot\n",
    "from  matplotlib import pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import cv2\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import numpy\n",
    "import numpy as np\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.metrics import mean_squared_error\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import pylab as plt\n",
    "import os\n",
    "from PIL import Image\n",
    "import csv\n",
    "from itertools import zip_longest\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "#set directories\n",
    "\n",
    "if not os.path.exists('RawImage_daily11am'):\n",
    "    os.makedirs('RawImage_daily11am')\n",
    "\n",
    "if not os.path.exists('RawImage_daily12pm'):\n",
    "    os.makedirs('RawImage_daily12pm')\n",
    "\n",
    "if not os.path.exists('RawImage_daily1pm'):\n",
    "    os.makedirs('RawImage_daily1pm')\n",
    "\n",
    "if not os.path.exists('RawImage_daily2pm'):\n",
    "    os.makedirs('RawImage_daily2pm')\n",
    "\n",
    "\n",
    "output_dir19 = r\"RawImage_daily11am\"\n",
    "output_dir20 = r\"RawImage_daily12pm\"\n",
    "output_dir21 = r\"RawImage_daily1pm\"\n",
    "output_dir22 = r\"RawImage_daily2pm\"\n",
    "\n",
    "mypath=r\"ClearImages_daily\"\n",
    "#mypath = sorted(Path(mypath).iterdir(mypath), key=os.path.getmtime)\n",
    "\n",
    "onlyfiles = [ f for f in listdir(mypath) if isfile(join(mypath,f)) ]\n",
    "\n",
    "\n",
    "\n",
    "images = numpy.empty(len(onlyfiles), dtype=object)\n",
    "\n",
    "x=[]\n",
    "y=[]\n",
    "z=[]\n",
    "\n",
    "for n in range(0, len(onlyfiles)):\n",
    "   \n",
    "    \n",
    "    fili=  join(mypath,onlyfiles[n])\n",
    "    \n",
    "    #fili_ref =  join(mypath,onlyfiles[n+1])\n",
    "        \n",
    "        \n",
    "    image= cv2.imread(fili,1)\n",
    "   \n",
    "    ##############Creating Image Title and labels based on YYYYDDMM and HHmm\n",
    "\n",
    "    file_fili = str(onlyfiles[n])\n",
    "\n",
    "    position = file_fili.index(\".jpg\")\n",
    "    \n",
    "    filename1 = file_fili[0:position]\n",
    "    \n",
    "    mm= ( filename1[20]+filename1[21])\n",
    "    hh= ( filename1[18]+filename1[19])\n",
    "    DD= ( filename1[16]+filename1[17])\n",
    "    MM= ( filename1[14]+filename1[15])\n",
    "    YYYY= ( filename1[10]+filename1[11]+filename1[12]+filename1[13])\n",
    "    fili_datelabel1= (\"YYYYMMDD\"+ \": \" + YYYY+MM+DD+\"-HHMM\"+\": \"+hh+mm)\n",
    "    \n",
    "    \n",
    "    im = Image.fromarray(image)\n",
    "\n",
    "    flag1= str(19)\n",
    "    flag2= str(20)\n",
    "    flag3= str(21)\n",
    "    flag4= str(22)\n",
    "   \n",
    "    print (hh)\n",
    "    if hh==flag1:\n",
    "    #if hh==flag1 or DD==flag2 or DD==flag3:\n",
    "       \n",
    "        im.save(output_dir19+'//' +filename1+'.jpg')\n",
    "    elif hh==flag2:\n",
    "        im.save(output_dir20+'//' +filename1+'.jpg')\n",
    "        \n",
    "    elif hh==flag3:\n",
    "        im.save(output_dir21+'//' +filename1+'.jpg')\n",
    "    elif hh==flag4:\n",
    "        im.save(output_dir22+'//' +filename1+'.jpg')\n",
    "  \n",
    "            \n",
    "    else:\n",
    "        print (\"Do not save those images\")\n",
    "print (\"task is finished\")     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39262ea4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2744cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import the necessary packages\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import argparse\n",
    "#import imutils\n",
    "import cv2\n",
    "#from imutils.paths import list_images\n",
    "import glob\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import numpy\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "#import rasterio\n",
    "#import cartopy\n",
    "#from scipy import ndimage\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "import os\n",
    "\n",
    "if not os.path.exists('filtered_images_noFog'):\n",
    "    os.makedirs('filtered_images_noFog')\n",
    "\n",
    "if not os.path.exists('filtered_images_Fog'):\n",
    "    os.makedirs('filtered_images_Fog')\n",
    "\n",
    "if not os.path.exists('daily_images_classified'):\n",
    "    os.makedirs('daily_images_classified')\n",
    "\n",
    "#setting required file directories\n",
    "No_Fogg_path=\"filtered_images_noFog\"\n",
    "Foggy_Path=\"filtered_images_Fog\"\n",
    "dailyimages=\"daily_images_classified\"\n",
    "\n",
    "modelpath=\"foggy_not_foggy.model\"\n",
    "########\n",
    "\n",
    "#mypath= r'dataset_imagery'\n",
    "mypath = os.getcwd() + \"\\\\summer2021\"\n",
    "\n",
    "onlyfiles = [ f for f in listdir(mypath) if isfile(join(mypath,f)) ]\n",
    "images = numpy.empty(len(onlyfiles), dtype=object)\n",
    "#Run the program\n",
    "output_dir=r\"figs\"\n",
    "\n",
    "for n in range(0, len(onlyfiles)):\n",
    "    print (\"Now Processing Image Number\", n+1 )\n",
    "\n",
    "    #images[n] =  join(mypath,onlyfiles[n])\n",
    "        #image = cv2.imread(images[n])\n",
    "        #im_med = ndimage.median_filter(image, 5)\n",
    "    image = cv2.imread(join(mypath,onlyfiles[n]))\n",
    "    orig = image.copy()\n",
    "  # pre-process the image for classification\n",
    "    #image= cv2.resize(image, (28, 28))\n",
    "    #resize image without distortion\n",
    "    img=image\n",
    "    size=(28,28)\n",
    "    h, w = img.shape[:2]\n",
    "    c = img.shape[2] if len(img.shape)>2 else 1\n",
    "    if h == w:\n",
    "      cv2.resize(img, size, cv2.INTER_AREA)\n",
    "    dif = h if h > w else w\n",
    "    \n",
    "    interpolation = cv2.INTER_AREA if dif > (size[0]+size[1])//2 else  cv2.INTER_CUBIC\n",
    "    x_pos = (dif - w)//2\n",
    "    y_pos = (dif - h)//2\n",
    "    \n",
    "    if len(img.shape) == 2:\n",
    "      mask = np.zeros((dif, dif), dtype=img.dtype)\n",
    "      mask[y_pos:y_pos+h, x_pos:x_pos+w] = img[:h, :w]\n",
    "    else:\n",
    "      mask = np.zeros((dif, dif, c), dtype=img.dtype)\n",
    "      mask[y_pos:y_pos+h, x_pos:x_pos+w, :] = img[:h, :w, :]\n",
    "      image=cv2.resize(mask, size, interpolation)\n",
    "\n",
    "\t########\n",
    "    image = img_to_array(image)\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    image = image.astype(\"float\") / 255.0\n",
    "    print (join(mypath,onlyfiles[n]))\n",
    "  # load the trained convolutional neural network\n",
    "    print(\"[INFO] loading network...\", onlyfiles[n])\n",
    "    model = load_model(modelpath)\n",
    "       \n",
    "    # classify the input image\n",
    "    (not_foggy, foggy) = model.predict(image)[0]\n",
    "    label = \"NotFoggy\" if not_foggy  > foggy else \"foggy\"\n",
    "    proba = not_foggy  if not_foggy > foggy  else foggy \n",
    "    label = \"{}: {:.2f}%\".format(label, proba * 100)\n",
    "  #print (label)\n",
    "  #output = imutils.resize(orig, width=1600)\n",
    "    cv2.putText(orig, label, (40, 100),  cv2.FONT_HERSHEY_SIMPLEX,0.7, (0, 255, 0), 2)\n",
    "    im = Image.fromarray(orig)\n",
    "    \n",
    "    file = str(onlyfiles[n])\n",
    "\n",
    "    position = file.index(\".jpg\")\n",
    "\n",
    "    filename = file[0:position]\n",
    "    \n",
    "    if (proba > 0.99 and not_foggy  > foggy):\n",
    "        label=\"Not Foggy\"\n",
    "      #im.save(No_Fogg_path+'//' +onlyfiles[n]+'-'+str(proba)+'.jpeg') \n",
    "        im.save(No_Fogg_path+'//' +str(proba)+'-'+filename+'.jpg') \n",
    "        im.save(dailyimages+ '//' +filename+ '.jpg')\n",
    "      \n",
    "\n",
    "    else:\n",
    "        label=\"Foggy\"\n",
    "      #im.save(Foggy_Path+'//' +onlyfiles[n]+'-'+str(proba)+'.jpeg') \n",
    "        im.save(Foggy_Path+'//' +str(proba)+'-'+filename+'.jpg') \n",
    "      \n",
    "\n",
    "        imgplot = plt.imshow(im)\n",
    "        plt.show() \n",
    "        continue\n",
    "\n",
    "        print (\"No more files left to process\")\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
